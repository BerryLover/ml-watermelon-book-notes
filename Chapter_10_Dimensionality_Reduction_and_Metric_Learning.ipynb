{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjnGXD558EG2XUmpgTH04K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 10 - Dimensionality Reduction and Metric Learning\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.*\n",
        "\n",
        "\n",
        "### 10.3 Principal Component Analysis\n",
        "#### 10.3.1 Simplified covariance in formula 10.15\n",
        "Consider $m$ samples in $d$-dimensional space, the data matrix $\\mathbf{X}=[\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_m}] \\in \\mathbb{R^{d \\times m}}$.\n",
        "\n",
        "Normally, the covariance matrix of $\\mathbf{X}$ is\n",
        "$$\\mathbf{\\Sigma} = \\frac{1}{m-1} \\mathbf{X} \\mathbf{X}^{\\mathrm{T}}$$\n",
        "\n",
        "<br>But the book omits the unbiased normalization factor $\\frac{1}{m-1}$ in the projection transformation of the covariance matrix: $\\mathbf{W}^{\\mathrm{T}} \\mathbf{X} \\mathbf{X}^{\\mathrm{T}} \\mathbf{W}$.\n",
        "<br>In PCA, MDS, or SVD, the scaling factor $\\frac{1}{n-1}$ is often omitted because:\n",
        "- It doesn't affect the eigenvectors (directions of variance)\n",
        "- It only scales the eigenvalues (magnitudes of variance)\n",
        "\n",
        "So for dimensionality reduction, the structure is preserved even without normalization.\n",
        "\n",
        "<br>And also, the data is mean-centered before the transformation so the mean vector $\\boldsymbol{\\mu}$ is not included in the projection."
      ],
      "metadata": {
        "id": "H5S7DsC6Qth2"
      }
    }
  ]
}