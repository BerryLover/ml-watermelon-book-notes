{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzokOeTvY4eKCd65qaGfma"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 10 - Dimensionality Reduction and Metric Learning\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.*\n",
        "\n",
        "\n",
        "### 10.3 Principal Component Analysis\n",
        "#### 10.3.1 Simplified covariance in formula 10.15\n",
        "Consider $m$ samples in $d$-dimensional space, the data matrix $\\mathbf{X}=[\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_m}] \\in \\mathbb{R^{d \\times m}}$.\n",
        "\n",
        "Normally, the covariance matrix of $\\mathbf{X}$ is\n",
        "$$\\mathbf{\\Sigma} = \\frac{1}{m-1} \\mathbf{X} \\mathbf{X}^{\\top}$$\n",
        "\n",
        "<br>But the book omits the unbiased normalization factor $\\frac{1}{m-1}$ in the projection transformation of the covariance matrix: $\\mathbf{W}^{\\top} \\mathbf{X} \\mathbf{X}^{\\top} \\mathbf{W}$, ($\\mathbf{W} = [\\mathbf{w}_1, \\mathbf{w}_2, ..., \\mathbf{w}_{d'}] \\in \\mathbb{R}^{d \\times d'}$).\n",
        "<br>In PCA, MDS, or SVD, the scaling factor $\\frac{1}{n-1}$ is often omitted because:\n",
        "- It doesn't affect the eigenvectors (directions of variance)\n",
        "- It only scales the eigenvalues (magnitudes of variance)\n",
        "\n",
        "So for dimensionality reduction, the structure is preserved even without normalization.\n",
        "\n",
        "<br>And also, the data is mean-centered before the transformation so the mean vector $\\boldsymbol{\\mu}$ is not included in the projection.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 10.3.2 Formula 10.16\n",
        "Formula 10.16 is a constrained optimization problem. The constraint $\\mathbf{W^{\\top}}\\mathbf{W} = \\mathbf{I}$ reflects that $\\mathbf{W}$ is an orthonormal / orthogonal matrix, where $\\mathbf{w}^{\\top}_{i} \\mathbf{w}_{i} = 1$ and $\\mathbf{w}^{\\top}_{i} \\mathbf{w}_{j} = 0 \\; (i \\neq j)$.\n",
        "<br>This ensures:\n",
        "- The columns of $\\mathbf{W}$ (i.e., the principal components) are orthogonal (uncorrelated) and unit-length (normalized), which avoids arbitrary scaling or rotation in the projection.\n",
        "- The variance interpretation is preserved. Without this constraint, you could artificially inflate the variance by scaling $\\mathbf{W}$, making the optimization meaningless. The constraint ensures that the variance captured is due to the direction, not the magnitude of the projection.\n",
        "- The problem fit into an optimization framework. With $\\mathbf{W^{\\top}}\\mathbf{W} = \\mathbf{I}$, the optimization becomes a constrained eigenvalue problem. The solution is given by the top-k eigenvectors of the covariance matrix $\\mathbf{X} \\mathbf{X}^{\\top}$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H5S7DsC6Qth2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the Coordinates of x in the New Basis W\n",
        "\n",
        "### 1. Form the Basis Matrix\n",
        "\n",
        "Let the new basis vectors be\n",
        "$w_1, w_2, \\dots, w_n \\;\\in\\; \\mathbb{R}^n$.\n",
        "\n",
        "Stack them as columns to form the matrix  \n",
        "$W \\;=\\; [\\,w_1 \\;\\; w_2 \\;\\;\\dots\\;\\; w_n\\,]\\;\\in\\;\\mathbb{R}^{n\\times n}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "### 2. Solve for x'\n",
        "\n",
        "We want scalars $(c_1,\\dots,c_n)$ such that $x \\;=\\; c_1\\,w_1 \\;+\\; c_2\\,w_2 \\;+\\;\\dots+\\;c_n\\,w_n$.\n",
        "  \n",
        "In matrix form,  \n",
        "$$\n",
        "x \\;=\\; W\\,x',\n",
        "\\quad\n",
        "x' \\;=\\;\n",
        "\\begin{bmatrix}\n",
        "c_1\\\\\n",
        "\\vdots\\\\\n",
        "c_n\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Case A: Invertible W\n",
        "\n",
        "If the columns of $W$ are linearly independent (so $W^{-1}$ exists), compute $x' \\;=\\; W^{-1}\\,x$.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Case B: Orthonormal Basis\n",
        "\n",
        "If ${w_i}$ is orthonormal, then $W^{-1} = W^\\top$ and $x' \\;=\\; W^\\top\\,x$.\n",
        "\n",
        "Note: distinguish between _orthonormal basis (标准正交基)_ and _orthogonal basis (正交基)_.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Case C: Non-square or Overcomplete \\(W\\)\n",
        "\n",
        "Use the pseudoinverse:  \n",
        "$\n",
        "x' \\;=\\; W^+\\,x\n",
        "\\;=\\;(W^\\top W)^{-1}W^\\top\\,x.\n",
        "$\n",
        "\n",
        "<br>\n",
        "\n",
        "### 3. Intuition\n",
        "\n",
        "- Applying $W^{-1}$ (or $W^\\top$ for orthonormal $W$) projects $x$ onto each basis vector $w_i$.  \n",
        "- The entries of $x'$ tell you exactly how much of each $w_i$ is needed to reconstruct $x$.\n",
        "\n"
      ],
      "metadata": {
        "id": "hPRvcgT6bwT9"
      }
    }
  ]
}