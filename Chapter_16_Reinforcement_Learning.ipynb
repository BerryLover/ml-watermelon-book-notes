{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5ADnv45oka4qNW7sFrTCQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 16 - Reinforcement Learning\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.* <br><br>\n",
        "\n",
        "\n",
        "### 16.3 Model-based Learning\n",
        "#### 16.3.1 Additional explanation on formula 16.7\n",
        "**Problem:** It is important to understand in derivation of formula 16.7, how step 2 is transformed into step 3 while still keeping the $\\mathbb{E}_\\pi$ in the last term, as demonstrated below.\n",
        "$$\\mathbb{E}_\\pi \\left[ \\frac{1}{T} r_1 + \\frac{T - 1}{T} \\cdot \\left( \\frac{1}{T - 1} \\sum_{t=2}^T r_t \\right) \\;\\middle|\\; x_0 = x \\right] \\\\\n",
        "=\\sum_{a \\in A} \\pi(x, a) \\sum_{x' \\in X} P^a_{x \\to x'} \\left( R^a_{x \\to x'} + \\frac{T - 1}{T} \\cdot \\mathbb{E}_\\pi \\left[ \\frac{1}{T-1} \\sum_{t=1}^{T-1} r_t \\;\\middle|\\; x_0 = x' \\right] \\right)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Derivation:**\n",
        "<br>The left formula can be splitted into two parts of expecations:\n",
        "$$\n",
        "\\frac{1}{T} \\mathbb{E}_\\pi[r_1 \\mid x_0 = x] + \\frac{T - 1}{T} \\mathbb{E}_\\pi \\left[ \\frac{1}{T - 1} \\sum_{t=2}^T r_t \\;\\middle|\\; x_0 = x \\right]\n",
        "$$\n",
        "\n",
        "<br>We focus only on the second term with expectation $\\mathbb{E}_\\pi \\left[ \\frac{1}{T - 1} \\sum_{t=2}^T r_t \\;\\middle|\\; x_0 = x \\right]$.\n",
        "\n",
        "With the _Law of Total Expectation_, we have\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[Y \\mid x_0 = x] = \\sum_{z} \\Pr(Z = z \\mid x_0 = x) \\cdot \\mathbb{E}[Y \\mid Z = z]\n",
        "$$\n",
        "\n",
        "In our case,\n",
        "- $Z=x_1$ (Let $x_1$ be the state after the first step, and because MDPs are Markov, the rest of the trajectory only depends on $x_1$.)\n",
        "- $Y = \\frac{1}{T - 1} \\sum_{t=2}^T r_t$\n",
        "\n",
        "and the formula above becomes\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_\\pi \\left[ \\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_0 = x \\right]\n",
        "&= \\sum_{x'} \\Pr(x_1 = x' \\mid x_0 = x) \\cdot \\mathbb{E}_\\pi[\\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_1 = x', x_0 = x]  \\\\\n",
        "& \\begin{aligned}\n",
        "    &\\text{And due to the Markov propertyï¼Œ} \\\\\n",
        "\\end{aligned} \\\\\n",
        "&= \\sum_{x'} \\Pr(x_1 = x' \\mid x_0 = x) \\cdot \\mathbb{E}_\\pi[\\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_1 = x']\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "<br>Applying the _Law of Total Probability_ on $\\Pr(x_1 = x' \\mid x_0 = x)$, the transition probability from $x_0=x$ to $x_1=x'$ under policy $\\pi$ is:\n",
        "\n",
        "$$\\Pr(x_1 = x' \\mid x_0 = x) = \\sum_{a \\in A} \\pi(x, a) \\; P^a_{x \\to x'}$$\n",
        "\n",
        "Finally, we get\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_\\pi \\left[ \\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_0 = x \\right]\n",
        "&= \\sum_{x' \\in X} (\\sum_{a \\in A} \\pi(x, a) \\; P^a_{x \\to x'}) \\cdot \\mathbb{E}_\\pi[\\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_1 = x']  \\\\\n",
        "&= \\sum_{a \\in A} \\pi(x, a) \\sum_{x' \\in X} P^a_{x \\to x'} \\cdot \\mathbb{E}_\\pi[\\frac{1}{T - 1} \\sum_{t=2}^T r_t \\mid x_1 = x']  \\\\\n",
        "& \\begin{aligned}\n",
        "    &\\text{Now shift the timeline. From the point of view of $x_1 = x'$,} \\\\\n",
        "    &\\text{the remaining rewards from step $2$ to $T$ are just another} \\\\\n",
        "    &\\text{$T-1$ step value function starting at $x'$.}\n",
        "\\end{aligned} \\\\\n",
        "&= \\sum_{a \\in A} \\pi(x, a) \\sum_{x' \\in X} P^a_{x \\to x'} \\cdot \\mathbb{E}_\\pi[\\frac{1}{T - 1} \\sum_{t=1}^{T-1} r_t \\mid x_0 = x']\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "<br><br>\n",
        "  \n",
        "#### 16.3.2 State Value Function $V(s)$ and State-Action Value Function $Q(s, a)$\n",
        "**Problem:** To understand the connection between $V(s)$ and $Q(s, a)$.\n",
        "\n",
        "**Formula:**\n",
        "<br>\n",
        "- State value function: $V(s) = \\mathbb{E}[R_t \\mid S_t = s]$\n",
        "<br>It is the expection of rewards under state $s$ when following policy $\\pi$.\n",
        "- State-action value function: $Q(s, a) = \\mathbb{E}[R_t \\mid S_t = s, A_t = a]$\n",
        "<br>It is the expection of rewards under state $s$ and take action $a$ when following policy $\\pi$.\n",
        "\n",
        "<br>Connection: $V(s) = \\sum_a \\pi(a \\mid s) \\cdot Q(s, a)$\n"
      ],
      "metadata": {
        "id": "nIGJFroKeHCd"
      }
    }
  ]
}