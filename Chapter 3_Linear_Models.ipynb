{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3 - Linear Models\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.*\n",
        "\n",
        "### 3.4 Linear Discriminant Analysis\n",
        "#### 3.4.1 Derivation of the components in formula (3.32)\n",
        "**Problem:** As described in the book, Linear Discriminant Analysis (LDA) maps a set of points from a high dimentional space $\\mathbb{R^n}$ to a lower dimensional space $\\mathbb{R}$ by linear transformation $\\mathbf{w^T}$. It is important to understand how the mean and variance change after the linear transformation.\n",
        "\n",
        "<br>**Formula:**\n",
        "\n",
        "***From ML Perspective***:\n",
        "<br>Consider a set of samples with $n$ features $\\mathbf{S}=\\{\\mathbf{x}_i | \\mathbf{x}_i \\in \\mathbb{R^n}\\}_{i=1}^{k}$, where $\\mathbf{x}_i = [x_{1i}, x_{2i}, ..., x_{ni}]^{T}$, each entry in the column vector $\\mathbf{x}_i$ is a realization (observation) of the corresponding feature.\n",
        "\n",
        "The mean vector of the set is $\\boldsymbol{\\mu_X}=\\frac{1}{k}\\sum_{i=1}^{k}\\mathbf{x}_i=[\\mu_{1}, \\mu_{2}, ..., \\mu_{n}]^{T}$, $\\boldsymbol{\\mu_X} \\in \\mathbb{R^n}$.\n",
        "\n",
        "The covariance matrix is $\\mathbf{\\Sigma_{X}}=(a_{pq})_{n \\times n}\n",
        "=\\begin{bmatrix}\n",
        "a_{11} & a_{12} & \\dots & a_{1n} \\\\\n",
        "a_{21} & a_{22} & \\dots & a_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n1} & a_{n2} & \\dots & a_{nn}\n",
        "\\end{bmatrix}$, where $a_{pq}=\\frac{1}{k-1}\\sum_{i=1}^{k} [(x_{pi}-\\mu_{p})(x_{qi}-\\mu_{q})]$.\n",
        "\n",
        "<br>When applying the linear transformation $Y=\\mathbf{w^{T} X}$ on set $\\mathbf{S}$, we obtain a scalar random variable $Y$. The mean vector becomes $\\mu_Y=\\mathbf{w^T}\\boldsymbol{\\mu_X}$, and the covariance matrix becomes $\\Sigma_{Y}=\\mathbf{w^{T}}\\mathbf{\\Sigma_X}\\mathbf{w}$.\n",
        "\n",
        "<br>***From Statistics Perspective***:\n",
        "<br>Consider each sample from set $\\mathbf{S}$ as a realization (observation) of the random vector (or (n)-dimensional random variable) $\\mathbf{X}=[X_1, X_2, ..., X_n]^{T}$. The mean vector and covariance matrix are $\\boldsymbol{\\mu_X}$ and $\\mathbf{\\Sigma_X}$ respectively.\n",
        "\n",
        "<br>After the linear transformation $Y=\\mathbf{w^{T} X}$, the mean and covariance change to $\\mu_Y=\\mathbf{w^T}\\boldsymbol{\\mu_X}$ and $\\Sigma_{Y}=\\mathbf{w^{T}}\\mathbf{\\Sigma_X}\\mathbf{w}$.\n",
        "\n",
        "<br>**Derivation (Statistics Perspective):** Consider a random vector $\\mathbf{X}=[X_1, X_2, ..., X_n]^{T}$. Each component $X_i$ is a random variable (or a feature variable in ML), and together they form the joint distribution over $\\mathbb{R}^n$.\n",
        "\n",
        "Linear transformation $Y=\\mathbf{w^{T} X}$ maps the random vector $\\mathbf{X}$ to a scalar random variable $Y$ with a constant linear operator $\\mathbf{w^{T}}$.\n",
        "\n",
        "For mean, we have\n",
        "$$\\mathbb{E}[\\mathbf{X}]=\\boldsymbol{\\mu_X},\\quad \\boldsymbol{\\mu_X} \\in \\mathbb{R^n}$$\n",
        "\n",
        "$$\\mathbb{E}[Y]\n",
        "=\\mathbb{E}[\\mathbf{w^{T}X}]\n",
        "=\\mathbf{w^{T}}\\mathbb{E}[\\mathbf{X}]\n",
        "=\\mathbf{w^{T}}\\boldsymbol{\\mu_X}$$\n",
        "\n",
        "For variance, the covariance matrix of random vector $\\mathbf{X}$ is defined as:\n",
        "$$\\operatorname{Var}(\\mathbf{X})\n",
        "=\\operatorname{Cov}(\\mathbf{X}, \\mathbf{X})\n",
        "=\\mathbf{\\Sigma_X}\n",
        "=\\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])^{T}]$$\n",
        "\n",
        "We can derive the covariance matrix of random variable $Y$ as:\n",
        "\n",
        "$\\operatorname{Cov}(Y, Y)\n",
        "=\\operatorname{Cov}(\\mathbf{w^{T} X}, \\mathbf{w^{T} X})\n",
        "\\\\=\\mathbb{E}[(\\mathbf{w^{T} X}-\\mathbb{E}[\\mathbf{w^{T}  X}])(\\mathbf{w^{T} X}-\\mathbb{E}[\\mathbf{w^{T}  X}])^{T}]\n",
        "\\\\=\\mathbb{E}[(\\mathbf{w^{T} X}-\\mathbf{w^{T}}\\mathbb{E}[\\mathbf{X}])(\\mathbf{w^{T} X}-\\mathbf{w^{T}}\\mathbb{E}[\\mathbf{X}])^{T}]\n",
        "\\\\=\\mathbb{E}[\\mathbf{w^{T}}(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{w^{T}}(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}]))^{T}]\n",
        "\\\\=\\mathbb{E}[\\mathbf{w^{T}}(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])^{T}\\mathbf{w}]\n",
        "\\\\=\\mathbf{w^{T}}\\mathbb{E}[(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])(\\mathbf{X}-\\mathbb{E}[\\mathbf{X}])^{T}]\\mathbf{w}\n",
        "\\\\=\\mathbf{w^{T}}\\operatorname{Cov}(\\mathbf{X}, \\mathbf{X})\\mathbf{w}\n",
        "\\\\=\\mathbf{w^{T}}\\mathbf{\\Sigma_X}\\mathbf{w}\n",
        "$\n",
        "\n",
        "<br>From the above derivation we get:\n",
        "$$\\operatorname{Var}(Y)\n",
        "=\\operatorname{Cov}(Y, Y)\n",
        "=\\Sigma_Y=\\mathbf{w^{T}}\\operatorname{Cov}(\\mathbf{X}, \\mathbf{X})\\mathbf{w}=\\mathbf{w^{T}}\\mathbf{\\Sigma_X}\\mathbf{w}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "AaqY_UPU0pqS"
      }
    }
  ]
}