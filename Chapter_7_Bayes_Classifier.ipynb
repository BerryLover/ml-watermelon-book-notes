{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 7 - Bayes Classifier\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.*\n",
        "\n",
        "\n",
        "### 7.2 Maximum Likelihood Estimation\n",
        "#### 7.2.1 Likelihood function, cross-entropy loss, and KL-divergence\n",
        "**Problem:**\n",
        "\n",
        "**Background:**\n",
        "\n",
        "1. **Shannon information / self-information:**\n",
        "<br>From a statistical perspective, information quantifies the reduction of uncertainty following an observation of a random variable X. In other words, it measures the surprisal of an event: the less likely an event is, the more information it conveys when it occurs.\n",
        "<br><br>If X is a discrete random variable with probability mass function $p(x) := \\mathbb{P}(X = x)$, and it takes values from set $\\mathcal{X}$. Then the self-information of observing $X = x$ is defined as:\n",
        "$$\\mathrm{I}(X = x) = \\log \\frac{1}{p(x)} = -\\log p(x)$$\n",
        "\n",
        "2. **Entropy:**\n",
        "<br>Entropy quantifies the average level of uncertainty in a random variable's outcomes. It measures how unpredictable or \"surprising\" the value of a variable is, on average, based on its probability distribution.\n",
        "<br><br>It is defined as the expectation of self-information:\n",
        "$$\\mathrm{H}(X)\n",
        ":= \\mathbb{E}[\\mathrm{I}(X)]\n",
        "= \\mathbb{E}[-\\log p(X)]\n",
        "= -\\sum_{x \\in \\mathcal{X}} p(x) \\log p(x)$$\n",
        "Entropy reaches its maximum when all outcomes are equally likely (i.e., the distribution is uniform), reflecting maximal uncertainty. Conversely, if one outcome is certain, entropy is zero, and there is no surprise in the result.\n"
      ],
      "metadata": {
        "id": "R_C4pUvt0guj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Notes\n",
        "\n",
        "### Formula Representation on Expectation\n",
        "$$\\mathrm{H}(X)\n",
        "= \\mathbb{E}[-\\log p(X)]\n",
        "= \\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$$\n",
        "\n",
        "<br>Note that both $\\mathbb{E}[-\\log p(X)]$ and $\\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$ are the correct forms of representation.\n",
        "\n",
        "<br>$\\mathbb{E}[-\\log p(X)]$ --> **Random-variable form**\n",
        "- Take the expectation of $-\\log p(X)$, where $X$ is the random variable. No subscript to declare the dummy — so $X$ stays the random variable throughout.\n",
        "\n",
        "<br>$\\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$ --> **Sampling-notation form**\n",
        "- Draw a sample $x$ from distribution $p$, then compute $-\\log p(x)$, and average. In the subscript $x \\sim p(x)$, $x$ is just a dummy variable (like the $x$ in $\\int f(x)\\,dx$). We consistently use that same symbol inside the brackets.\n",
        "- Why lowercase $x$ inside the expectation?\n",
        "<br>**Dummy-variable convention:** In calculus we write $\\int f(t)\\,dt$ or $\\sum_i a_i$. The symbol $t$ or $i$ is bound—it lives only inside the integral or sum. Here, $x$ plays the same role: a placeholder inside the expectation.  Using lowercase stresses it's not the capital-letter random variable $X$ we might refer to elsewhere in our text; it's just the \"current draw\" from $p$.\n",
        "- Dummy variables can live inside $\\mathbb{E}[\\cdot]$. Because $\\mathbb{E}[\\cdot]$ is defined by a sum or an integral, any symbol we choose inside—so long as it matches the subscript declaration—is perfectly valid:\n",
        "$$\\mathbb{E}_{u \\sim p(u)}[g(u)]\n",
        "\\quad\\text{or}\\quad\n",
        "\\sum_{u} g(u)\\,p(u)$$\n",
        "And something like $\\mathbb{E}_{z\\sim p(z)}[z^2]$, which means “sum/integrate $z^2\\,p(z)$.”\n",
        "- $\\mathbb{E}_{x\\sim p(x)}[f(x)]$ is just a notational shortcut for the underlying sum/integral. The lowercase $x$ is a bound dummy variable—like the $t$ in $\\int f(t)\\,dt$. We can swap $x$ for any letter, what matters is that the subscript and the bracketed expression agree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "86FBYjK7l6U8"
      }
    }
  ]
}