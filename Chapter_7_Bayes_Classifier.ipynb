{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfyUrZiB06AuLOL2gRvAnT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 7 - Bayes Classifier\n",
        "\n",
        "_**Author:** Zitong Su_\n",
        "\n",
        "*This note includes some formula derivations (and/or extended materials) additional to the __Machine Learning (\"Watermelon Book\")__ or the __Pumpkin Book__.*\n",
        "\n",
        "\n",
        "### 7.2 Maximum Likelihood Estimation\n",
        "#### 7.2.1 Likelihood function, cross-entropy loss, and KL-divergence\n",
        "**Problem:**\n",
        "In Maximum Likelihood Estimation (MLE), the likelyhood function is constructed as:\n",
        "$$\n",
        "L(\\theta \\mid \\mathbf{x}) \\;=\\; p(\\mathbf{x} \\mid \\theta) \\;=\\; \\prod_{i=1}^{n} p(x_i; \\theta) \\\\\n",
        "\\mathbf{x} = (x_1,\\dots,x_n),\n",
        "\\quad\n",
        "$$\n",
        "<br>We would like to prove that optimizing the likelihood function indeed converges to the real probability distribution.\n",
        "\n",
        "<br>**Background:**\n",
        "\n",
        "1. **Shannon information / self-information:**\n",
        "<br>From a statistical perspective, _information_ quantifies the reduction of uncertainty following an observation of a random variable X. In other words, it measures the surprisal of an event: the less likely an event is, the more information it conveys when it occurs.\n",
        "<br><br>If X is a discrete random variable with probability mass function $p(x) := \\mathbb{P}(X = x)$, and it takes values from set $\\mathcal{X}$. Then the self-information of observing $X = x$ is defined as:\n",
        "$$\\mathrm{I}(X = x) = \\log \\frac{1}{p(x)} = -\\log p(x)$$\n",
        "\n",
        "2. **Entropy:**\n",
        "<br>Entropy quantifies the average level of uncertainty in a random variable's outcomes. It measures how unpredictable or \"surprising\" the value of a variable is, on average, based on its probability distribution.\n",
        "<br><br>It is defined as the expectation of self-information:\n",
        "$$\n",
        "\\mathrm{H}(X) = \\mathrm{H}(p) := \\mathbb{E}[\\mathrm{I}(X)] = \\mathbb{E}[-\\log p(X)] =\n",
        "\\begin{cases}\n",
        "-\\sum\\limits_{x \\in \\mathcal{X}} p(x) \\log p(x), & \\text{for a discrete distribution} \\\\\n",
        "-\\int_{\\mathcal{X}} p(x) \\log p(x) \\, dx, & \\text{for a continuous distribution}\n",
        "\\end{cases}\n",
        "$$\n",
        "<br>Entropy reaches its maximum when all outcomes are equally likely (i.e., the distribution is uniform), reflecting maximal uncertainty. Conversely, if one outcome is certain, entropy is zero ($\\lim_{p(x) \\to 0} [p(x) \\log p(x)]=0$ proof with L'Hôpital's rule), and there is no surprise in the result.\n",
        "\n",
        "3. **Relative entropy / KL divergence:**\n",
        "<br>KL divergence measures how much a model's probability distribution $q(x)$ is different from the true probability distribution $p(x)$. Usually, $p(x)$ represents the data, the observations, or a measured probability distribution. Distribution $q(x)$ represents instead a theory, a model, a description or an approximation of $p$ <sup>[1][1]</sup>.\n",
        "<br><br>For discrete $p$ and $q$:\n",
        "$$\n",
        "\\mathrm{D}_{\\text{KL}}(p \\parallel q)\n",
        "= -\\sum_{x \\in \\mathcal{X}} p(x) \\log \\left( \\frac{q(x)}{p(x)} \\right)\n",
        "= \\sum_{x \\in \\mathcal{X}} p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right)\n",
        "$$\n",
        "<br>For continuous $p$ and $q$:\n",
        "$$\\mathrm{D}_{\\text{KL}}(p \\parallel q)\n",
        "= -\\int_{\\mathcal{X}} p(x) \\log\\left( \\frac{q(x)}{p(x)} \\right) \\, dx\n",
        "= \\int_{\\mathcal{X}} p(x) \\log\\left( \\frac{p(x)}{q(x)} \\right) \\, dx\n",
        "$$\n",
        "<br>Key properties:\n",
        "<br>1) Asymmetry: $\\mathrm{D}_{\\text{KL}}(p \\parallel q) \\neq \\mathrm{D}_{\\text{KL}}(q \\parallel p)$\n",
        "<br>2) Non-negativity:  $\\mathrm{D}_{\\text{KL}}(p \\parallel q) \\ge 0$ (Gibbs' inequality), and $\\mathrm{D}_{\\text{KL}}(p \\parallel q) = 0$ only when $p(x) = q(x)$.\n",
        "<br>3) Not a true distance / metric: $\\mathrm{D}_{\\text{KL}}$ does not satisfy symmetry property and triangle inequality.\n",
        "<br>$\\mathrm{D}_{\\text{KL}}(p \\parallel q) \\neq \\mathrm{D}_{\\text{KL}}(q \\parallel p)$\n",
        "\n",
        "4. **Cross entropy:**\n",
        "<br>Cross entropy measures the difference between two probability distributions, typically a true distribution $p(x)$ and an estimated or predicted distribution $q(x)$. It tells us how many bits, on average, are needed to encode data from $p$ using a code optimized for $q$. The definition of corss entropy is as below:\n",
        "$$\n",
        "\\mathrm{H}(p, q)\n",
        "= \\mathbb{E}_{x \\sim p(x)}[-\\log q(x)]\n",
        "= -\\mathbb{E}_{x \\sim p(x)}[\\log q(x)]\n",
        "=\n",
        "\\begin{cases}\n",
        "-\\sum\\limits_{x \\in \\mathcal{X}} p(x) \\log q(x), & \\text{for a discrete distribution} \\\\\n",
        "-\\int_{\\mathcal{X}} p(x) \\log q(x) \\, dx, & \\text{for a continuous distribution}\n",
        "\\end{cases}\n",
        "$$\n",
        "<br>The relationship among entropy, relative entropy (KL divergence), and cross entropy is:\n",
        "$$\n",
        "\\mathrm{H}(p, q) = \\mathrm{H}(p) + \\mathrm{D}_{\\text{KL}}(p \\parallel q)\n",
        "$$\n",
        "\n",
        "5. **Additional materials:**\n",
        "<br>1) Claude Shannon and Warren Weaver first proposed the abovementioned information theory concepts in _The Mathematical Theory of Communication_ <sup>[2][2]</sup>. Please check out the publication for more information.\n",
        "<br>2) **Jensen-Shannon divergence:**\n",
        "The Jensen-Shannon divergence (JSD) is a symmetrized and smoothed version of the KL divergence:\n",
        "$$\n",
        "\\mathrm{JSD}(P \\parallel Q)\n",
        "= \\frac{1}{2} D_{\\mathrm{KL}}(P \\parallel M)\n",
        "+ \\frac{1}{2} D_{\\mathrm{KL}}(Q \\parallel M)\n",
        "\\quad \\text{where} \\quad\n",
        "M = \\frac{1}{2}(P + Q)\n",
        "$$\n",
        "\n",
        "\n",
        "<br>**Formula:**\n",
        "<br>For Likelihood Function:\n",
        "$$\n",
        "L(\\theta \\mid \\mathbf{x}) \\;=\\; p(\\mathbf{x} \\mid \\theta) \\;=\\; \\prod_{i=1}^{n} p(x_i; \\theta) \\\\\n",
        "\\mathbf{x} = (x_1,\\dots,x_n),\n",
        "\\quad\n",
        "$$\n",
        "\n",
        "\n",
        "<br>**Derivation:**\n",
        "<br>We will show that **maximizing the Likelihood Function in MLE is equivalent to minimizing the relative entropy (KL divergence)**. We only deduct on the discrete distribution. The same applies to the continuous distribution.\n",
        "<br>From KL divergence, we can derive as follows:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\arg\\min_{q} \\, \\mathrm{D}_{\\mathrm{KL}}(p \\parallel q)\n",
        "&= \\arg\\min_{q} \\, [\\mathrm{H}(p, q) - \\mathrm{H}(p)] \\\\\n",
        "&= \\arg\\min_{q} \\, [-\\mathbb{E}_{p}[\\log q(x)] + \\mathbb{E}_{p}[\\log p(x)]] \\\\\n",
        "&\\text{and according to the Law of Large Numbers}\\\\\n",
        "&= \\arg\\min_{q} [\\left( -\\frac{1}{n} \\sum_{i=1}^{n} \\log q(x_i) \\right) + C] \\\\\n",
        "&\\Leftrightarrow \\arg\\max_{q} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\log q(x_i) \\right) \\\\\n",
        "&\\Leftrightarrow \\arg\\max_{q} \\sum_{i=1}^{n} \\log q(x_i) \\\\\n",
        "&\\Leftrightarrow \\arg\\max_{q} \\log \\left[ \\prod_{i=1}^{n} q(x_i) \\right] \\\\\n",
        "&\\Leftrightarrow \\arg\\max_{q} \\prod_{i=1}^{n} q(x_i)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "From the derivation, minimizing the relative entropy is equivalent to maximizing $\\prod_{i=1}^{n} q(x_i)$ with respect to the distribution $q$. Considering the assumption of independency among the observations. We have:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\arg\\max_{q} \\prod_{i=1}^{n} q(x_i)\n",
        "&\\Leftrightarrow \\arg\\max_{q} \\mathbb{Q}(x_1, x_2, ..., x_n)  \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "<br>Since relative entropy measures the difference between the model's distribution and the true distribution, minimizing the relative entropy, i.e., maximizing the likelihood function, finds us a way to converge to the true data distribution.\n",
        "\n",
        "\n",
        "[1]: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
        "[2]: https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication\n"
      ],
      "metadata": {
        "id": "R_C4pUvt0guj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Notes\n",
        "\n",
        "### Formula Representation on Expectation\n",
        "$$\\mathrm{H}(X)\n",
        "= \\mathbb{E}[-\\log p(X)]\n",
        "= \\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$$\n",
        "\n",
        "<br>Note that both $\\mathbb{E}[-\\log p(X)]$ and $\\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$ are the correct forms of representation.\n",
        "\n",
        "<br>$\\mathbb{E}[-\\log p(X)]$ --> **Random-variable form**\n",
        "- Take the expectation of $-\\log p(X)$, where $X$ is the random variable. No subscript to declare the dummy — so $X$ stays the random variable throughout.\n",
        "\n",
        "<br>$\\mathbb{E}_{x \\sim p(x)}[-\\log p(x)]$ --> **Sampling-notation form**\n",
        "- Draw a sample $x$ from distribution $p$, then compute $-\\log p(x)$, and average. In the subscript $x \\sim p(x)$, $x$ is just a dummy variable (like the $x$ in $\\int f(x)\\,dx$). We consistently use that same symbol inside the brackets.\n",
        "- Why lowercase $x$ inside the expectation?\n",
        "<br>**Dummy-variable convention:** In calculus we write $\\int f(t)\\,dt$ or $\\sum_i a_i$. The symbol $t$ or $i$ is bound—it lives only inside the integral or sum. Here, $x$ plays the same role: a placeholder inside the expectation.  Using lowercase stresses it's not the capital-letter random variable $X$ we might refer to elsewhere in our text; it's just the \"current draw\" from $p$.\n",
        "- Dummy variables can live inside $\\mathbb{E}[\\cdot]$. Because $\\mathbb{E}[\\cdot]$ is defined by a sum or an integral, any symbol we choose inside—so long as it matches the subscript declaration—is perfectly valid:\n",
        "$$\\mathbb{E}_{u \\sim p(u)}[g(u)]\n",
        "\\quad\\text{or}\\quad\n",
        "\\sum_{u} g(u)\\,p(u)$$\n",
        "And something like $\\mathbb{E}_{z\\sim p(z)}[z^2]$, which means “sum/integrate $z^2\\,p(z)$.”\n",
        "- $\\mathbb{E}_{x\\sim p(x)}[f(x)]$ is just a notational shortcut for the underlying sum/integral. The lowercase $x$ is a bound dummy variable—like the $t$ in $\\int f(t)\\,dt$. We can swap $x$ for any letter, what matters is that the subscript and the bracketed expression agree.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "86FBYjK7l6U8"
      }
    }
  ]
}